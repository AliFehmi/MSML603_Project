{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7JYanKbn0OV"
      },
      "source": [
        "Reading the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVZ9U6rKkuoR",
        "outputId": "2f2c365f-16c6-4101-d948-1a0eac3d1e9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File size: 47040000 bytes. Expected: 47040000 bytes.\n",
            "File size: 60000 bytes. Expected: 60000 bytes.\n",
            "File size: 7840000 bytes. Expected: 7840000 bytes.\n",
            "File size: 10000 bytes. Expected: 10000 bytes.\n",
            "Train Images Shape: (60000, 28, 28)\n",
            "Train Labels Shape: (60000,)\n",
            "Test Images Shape: (10000, 28, 28)\n",
            "Test Labels Shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "\n",
        "def load_images(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
        "        data = f.read()\n",
        "        print(f\"File size: {len(data)} bytes. Expected: {num_images * rows * cols} bytes.\")\n",
        "        images = np.frombuffer(data, dtype=np.uint8).reshape(num_images, rows, cols)\n",
        "    return images\n",
        "\n",
        "def load_labels(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
        "        data = f.read()\n",
        "        print(f\"File size: {len(data)} bytes. Expected: {num_labels} bytes.\")\n",
        "        labels = np.frombuffer(data, dtype=np.uint8)\n",
        "    return labels\n",
        "\n",
        "\n",
        "train_images_path = 'train-images.idx3-ubyte'\n",
        "train_labels_path = 'train-labels.idx1-ubyte'\n",
        "test_images_path = 't10k-images.idx3-ubyte'\n",
        "test_labels_path = 't10k-labels.idx1-ubyte'\n",
        "\n",
        "\n",
        "train_images = load_images(train_images_path)\n",
        "train_labels = load_labels(train_labels_path)\n",
        "test_images = load_images(test_images_path)\n",
        "test_labels = load_labels(test_labels_path)\n",
        "\n",
        "\n",
        "print(f\"Train Images Shape: {train_images.shape}\")\n",
        "print(f\"Train Labels Shape: {train_labels.shape}\")\n",
        "print(f\"Test Images Shape: {test_images.shape}\")\n",
        "print(f\"Test Labels Shape: {test_labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmEsguGEoEbN"
      },
      "source": [
        "Preprocess the data and initialize the weights & biases. I found He initialization works best and copied to my project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kawjAzn-xvw6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def preprocess_images(images):\n",
        "    # Flatten and normalize the images\n",
        "    num_images = images.shape[0]\n",
        "    flattened_images = images.reshape(num_images, -1)  # Flatten to (num_images, 28*28)\n",
        "    normalized_images = flattened_images / 255.0       # Normalize to range [0, 1]\n",
        "    return normalized_images\n",
        "def one_hot_encode(labels, num_classes=10):\n",
        "    # Ensure labels are integers\n",
        "    labels = labels.astype(int)\n",
        "    # Convert labels to one-hot encoded vectors\n",
        "    one_hot = np.zeros((labels.size, num_classes))\n",
        "    one_hot[np.arange(labels.size), labels] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Preprocess training and test data\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)\n",
        "train_labels = one_hot_encode(train_labels)\n",
        "test_labels = one_hot_encode(test_labels)\n",
        "def initialize_weights_and_biases_explicit(layer_sizes):\n",
        "    \"\"\"\n",
        "    Initialize weights and biases for a feedforward neural network with explicit access.\n",
        "    Args:\n",
        "        layer_sizes: List containing the number of neurons in each layer.\n",
        "    Returns:\n",
        "        params: Dictionary containing weights and biases for each layer.\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "    for i in range(1, len(layer_sizes)):\n",
        "        # Create keys for weights and biases for layer i\n",
        "        params[f\"W{i}\"] = np.random.randn(layer_sizes[i], layer_sizes[i-1]) * np.sqrt(2 / layer_sizes[i-1])  # He initialization\n",
        "        params[f\"b{i}\"] = np.zeros((layer_sizes[i],))  # Bias vector shape: (layer_sizes[i],)\n",
        "    return params\n",
        "\n",
        "# Define the network architecture\n",
        "layer_sizes = [784, 128, 64, 10]  # Input layer (784), 2 hidden layers (128, 64), output layer (10)\n",
        "\n",
        "# Initialize weights and biases\n",
        "params = initialize_weights_and_biases_explicit(layer_sizes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW8FRvj0qlYl"
      },
      "source": [
        "For hidden layers I used relu activation function and for the output layer I used softmax. I used cross-entropy as loss function. It takes around 4 minutes to complete depending on the num_epochs value however this was the only way to get consistently 95% or above accuracy. I did not use any external source here since the implementation is very familiar with what we had done in previous hws."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_YGJodm6uIO",
        "outputId": "f95ac05b-facb-4817-cb84-47e160ff74c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 96.27%\n"
          ]
        }
      ],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "def softmax(logits):\n",
        "    logits = logits - np.max(logits)\n",
        "    exp_scores = np.exp(logits)\n",
        "    return exp_scores / np.sum(exp_scores)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-9))\n",
        "\n",
        "# hyperparameters for the neural networks\n",
        "learning_rate = 0.01\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # Training of the neural network\n",
        "  for i in range(0, train_images.shape[0]):\n",
        "    train_image_vector = train_images[i]\n",
        "  # Feed forward\n",
        "    # Input Layer\n",
        "    z1 = params['W1'] @ train_image_vector + params['b1'] # Multiply the input with the W1 and add the bias, input for the first hidden layer\n",
        "    # First Hidden Layer\n",
        "    a1 = relu(z1) # Apply relu on the z1 which will be the output of the HL1.\n",
        "    z2 = params['W2'] @ a1 + params['b2'] # transform the a1 using the W2 and b2 to create the input for the HL2.\n",
        "    # Second Hidden Layer\n",
        "    a2 = relu(z2)\n",
        "    z3 = params['W3'] @ a2 + params['b3']\n",
        "\n",
        "    # Output Layer\n",
        "    a3 = softmax(z3[np.newaxis, :]).flatten()\n",
        "  # backward propagation\n",
        "    dz3 = a3 - train_labels[i] # da3 and dz3's respective derivatives simplifies to this\n",
        "    dW3 = np.outer(dz3, a2)\n",
        "    db3 = dz3\n",
        "    da2 = dz3 @ params['W3']\n",
        "\n",
        "    dz2 = da2 * relu_derivative(z2)\n",
        "    dW2 = np.outer(dz2, a1)\n",
        "    db2 = dz2\n",
        "\n",
        "    da1 = dz2 @ params['W2']\n",
        "    dz1 = da1 * relu_derivative(z1)\n",
        "    dW1 = np.outer(dz1, train_image_vector)\n",
        "    db1 = dz1\n",
        "  # Update weights and bias accordingly\n",
        "    params['W1'] -= learning_rate * dW1\n",
        "    params['W2'] -= learning_rate * dW2\n",
        "    params['W3'] -= learning_rate * dW3\n",
        "    params['b1'] -= learning_rate * db1\n",
        "    params['b2'] -= learning_rate * db2\n",
        "    params['b3'] -= learning_rate * db3\n",
        "\n",
        "# Testing neural network\n",
        "correct_predictions = 0\n",
        "for i in range(0, test_images.shape[0]):\n",
        "  test_image_vector = test_images[i]\n",
        "  z1 = params['W1'] @ test_image_vector + params['b1']\n",
        "  a1 = relu(z1)\n",
        "  z2 = params['W2'] @ a1 + params['b2']\n",
        "  a2 = relu(z2)\n",
        "  z3 = params['W3'] @ a2 + params['b3']\n",
        "  a3 = softmax(z3[np.newaxis, :]).flatten()  # Apply softmax\n",
        "  # Get predicted class (index of max probability)\n",
        "  predicted_class = np.argmax(a3)\n",
        "\n",
        "  # Get true class since its one hot encoded\n",
        "  true_class = np.argmax(test_labels[i])\n",
        "\n",
        "  # Check if prediction is correct\n",
        "  if predicted_class == true_class:\n",
        "      correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / test_images.shape[0]\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YjsCP48Jg4ZK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.functional import relu, softmax\n",
        "# Convert train_images and test_images to tensors\n",
        "train_images = torch.from_numpy(train_images).float().to(device)\n",
        "test_images = torch.from_numpy(test_images).float().to(device)\n",
        "train_labels = torch.from_numpy(train_labels).float().to(device)\n",
        "test_labels = torch.from_numpy(test_labels).float().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u-FnfqTMz7w",
        "outputId": "c6301c3d-8413-4807-a2d8-75ca51355f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/5, Loss: 411.7756\n",
            "Epoch 2/5, Loss: 237.4963\n",
            "Epoch 3/5, Loss: 220.8316\n",
            "Epoch 4/5, Loss: 212.0178\n",
            "Epoch 5/5, Loss: 202.2744\n",
            "Test Accuracy: 96.27%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "train_images = train_images.reshape( -1,28, 28)\n",
        "test_images = test_images.reshape( -1,28, 28)\n",
        "\n",
        "# Define the CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=0)  # 1 input channel, 16 output channels\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 10 * 10, 48)\n",
        "        self.fc2 = nn.Linear(48, 10)  # Output for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 32 * 10 * 10)  # Flatten\n",
        "        x = relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "num_batches = train_images.shape[0] // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        # Get mini-batch\n",
        "        batch_images = train_images[i * batch_size:(i + 1) * batch_size].unsqueeze(1)\n",
        "        batch_labels = train_labels[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_images)\n",
        "        loss = criterion(outputs, torch.argmax(batch_labels, dim=1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Testing loop\n",
        "model.eval()\n",
        "correct_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for i in range(test_images.shape[0]):\n",
        "        test_image = test_images[i].unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "        test_label = torch.argmax(test_labels[i])\n",
        "        output = model(test_image)\n",
        "        predicted_class = torch.argmax(output)\n",
        "        if predicted_class == test_label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_images)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mAkWpvzGPnh"
      },
      "source": [
        "OPTIONAL=> MY IMPLEMENTATION OF CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "J07PlAmi5lfo",
        "outputId": "84dc435f-0949-40c0-a944-789cdca6c0c2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def conv2d_no_channel(input_image, filter, stride, padding):\n",
        "    \"\"\"\n",
        "    Perform 2D convolution on an input image without channel dimension.\n",
        "    \"\"\"\n",
        "    # Add zero-padding to the input image\n",
        "    # if padding > 0:\n",
        "    #     input_image = np.pad(input_image, ((padding, padding), (padding, padding)), mode='constant')\n",
        "\n",
        "    # Get input and filter dimensions\n",
        "    H, W = input_image.shape\n",
        "    kH, kW = filter.shape\n",
        "\n",
        "    # Calculate output dimensions\n",
        "    H_out = (H - kH) // stride + 1\n",
        "    W_out = (W - kW) // stride + 1\n",
        "\n",
        "    # Initialize output feature map\n",
        "    feature_map = np.zeros((H_out, W_out))\n",
        "\n",
        "    # Perform convolution\n",
        "    for i in range(0, H_out):\n",
        "        for j in range(0, W_out):\n",
        "            region = input_image[i * stride:i * stride + kH, j * stride:j * stride + kW]\n",
        "            feature_map[i, j] = np.sum(region * filter)\n",
        "\n",
        "    return feature_map\n",
        "def max_pooling(feature_map, pool_size, stride):\n",
        "\n",
        "    H, W = feature_map.shape\n",
        "    H_out = (H - pool_size) // stride + 1\n",
        "    W_out = (W - pool_size) // stride + 1\n",
        "\n",
        "    pooled_map = np.zeros((H_out, W_out))\n",
        "\n",
        "\n",
        "    for i in range(0, H_out):\n",
        "        for j in range(0, W_out):\n",
        "            region = feature_map[i * stride:i * stride + pool_size, j * stride:j * stride + pool_size]\n",
        "            pooled_map[i, j] = np.max(region)\n",
        "\n",
        "    return pooled_map\n",
        "\n",
        "def softmax(logits):\n",
        "    logits = logits - np.max(logits)  # Numerical stability\n",
        "    exp_scores = np.exp(logits)\n",
        "    return exp_scores / np.sum(exp_scores)\n",
        "\n",
        "def dense_layer(input_vector, weights, bias):\n",
        "    return np.dot(weights, input_vector) + bias\n",
        "\n",
        "# initialize the parameters here\n",
        "# This is crucial because I want to get 95% accuracy so I have to change these values frequently.\n",
        "convstride1 = 1  # Smaller stride to preserve spatial details\n",
        "convpadding1 = 0  # No padding\n",
        "poolingsize1 = 2  # Standard pooling size\n",
        "poolingstride1 = 2  # Non-overlapping pooling\n",
        "\n",
        "convstride2 = 1  # Smaller stride for better feature extraction\n",
        "convpadding2 = 0  # No padding\n",
        "poolingsize2 = 2  # Standard pooling size\n",
        "poolingstride2 = 1  # Overlapping pooling\n",
        "\n",
        "filter1size = 3  # Larger filter to capture more patterns\n",
        "filter2size = 3  # Larger filter in the second layer\n",
        "\n",
        "# Calculate flattened_vector_size dynamically\n",
        "flattened_vector_size = 10 * 10  # Example output size of the second pooling layer (10x10 = 100)\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "# Initialize the weights & biases and the kernels\n",
        "filter1_shape = (filter1size, filter1size)\n",
        "filter2_shape = (filter2size, filter2size)\n",
        "\n",
        "# He initialization for convolutional kernels\n",
        "filter1 = np.random.randn(*filter1_shape) * np.sqrt(2 / np.prod(filter1_shape))\n",
        "filter2 = np.random.randn(*filter2_shape) * np.sqrt(2 / np.prod(filter2_shape))\n",
        "\n",
        "\n",
        "# Fully Connected Layer 1 (Flattened â†’ Hidden Layer)\n",
        "w1 = np.random.randn(48, flattened_vector_size) * np.sqrt(2 / flattened_vector_size)\n",
        "b1 = np.zeros(48)\n",
        "\n",
        "w2 = np.random.randn(10, 48) * np.sqrt(2 / 48)\n",
        "b2 = np.zeros(10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "  total_loss=0\n",
        "  for i in range(train_images.shape[0]):\n",
        "\n",
        "      train_image_matrix = train_images[i]\n",
        "      #train_image_matrix = Timage\n",
        "\n",
        "      # --- Forward Pass ---\n",
        "      # First convolutional layer\n",
        "\n",
        "      feature_map1 = conv2d_no_channel(train_image_matrix, filter1, stride = convstride1, padding = convpadding1)\n",
        "\n",
        "      feature_map1_activated = relu(feature_map1)\n",
        "\n",
        "      # First pooling layer\n",
        "      pooled_map1 = max_pooling(feature_map1_activated, pool_size=poolingsize1, stride=poolingstride1)\n",
        "\n",
        "      # Second convolutional layer\n",
        "      feature_map2 = conv2d_no_channel(pooled_map1, filter2, stride=convstride2, padding=convpadding2)\n",
        "\n",
        "      feature_map2_activated = relu(feature_map2)\n",
        "\n",
        "      # Second pooling layer\n",
        "      pooled_map2 = max_pooling(feature_map2_activated, pool_size=poolingsize1, stride=poolingstride2)\n",
        "\n",
        "      # Flatten the output for the neural network\n",
        "      flattened_vector = pooled_map2.flatten()\n",
        "\n",
        "\n",
        "      # Fully connected layer 1\n",
        "      fc1_output = dense_layer(flattened_vector, w1, b1)\n",
        "      fc1_activated = relu(fc1_output)\n",
        "\n",
        "      # Fully connected layer 2 (Output)\n",
        "      logits = dense_layer(fc1_activated, w2, b2)\n",
        "      output_probs = softmax(logits)\n",
        "       # Calculate loss\n",
        "      y_true = train_labels[i]\n",
        "      loss = -np.sum(y_true * np.log(np.clip(output_probs, 1e-15, 1.0)))\n",
        "\n",
        "      total_loss += loss  # Accumulate loss\n",
        "      # --- Backpropagation ---\n",
        "      y_true = train_labels[i]\n",
        "      dz2 = output_probs - y_true\n",
        "\n",
        "      dw2 = np.outer(dz2, fc1_activated)\n",
        "      db2 = dz2\n",
        "      da1 = np.dot(w2.T, dz2)\n",
        "      dz1 = da1 * relu_derivative(fc1_output)\n",
        "      dw1 = np.outer(dz1, flattened_vector)\n",
        "      db1 = dz1\n",
        "      # layer's backpropagation\n",
        "      d_input = np.dot(w1.T, dz1)\n",
        "      side_length = int(math.sqrt(flattened_vector.size))\n",
        "      d_input = d_input.reshape(side_length, side_length) # which is equal to\n",
        "      d_pooled_map2 = d_input\n",
        "      #print(d_pooled_map2.shape)\n",
        "      # max pooling 2 backpropagation\n",
        "      d_feature_map2_activated = np.zeros_like(feature_map2_activated)\n",
        "      for pl_row in range(0, d_pooled_map2.shape[0]):\n",
        "        for pl_column in range(0, d_pooled_map2.shape[1]):\n",
        "          # get the region from feature map2\n",
        "          start_row = pl_row * poolingstride2 # multiply it with stride\n",
        "          start_column = pl_column * poolingstride2\n",
        "          end_row = start_row + poolingsize2 # add the pool size to get the regions other end\n",
        "          end_column = start_column + poolingsize2\n",
        "          region = feature_map2_activated[start_row:end_row, start_column: end_column] # get the nxn column where we applied max pooling which corresponds to d_inputs[pl_row, pl_column]\n",
        "          #print(region.shape)\n",
        "          max_index = np.unravel_index(np.argmax(region), region.shape) # get the max index from to region\n",
        "          max_pos = (start_row + max_index[0], start_column + max_index[1]) # apply to the current indexes\n",
        "          # all we need to do assign the loss\n",
        "          d_feature_map2_activated[max_pos] += d_pooled_map2[pl_row, pl_column]\n",
        "      d_feature_map2 = relu_derivative(d_feature_map2_activated)\n",
        "\n",
        "      # convolution layer 2 backpropagation\n",
        "      d_pooled_map1 = np.zeros_like(pooled_map1)\n",
        "      d_filter2 = np.zeros_like(filter2)\n",
        "      for df2_row in range(0, d_feature_map2.shape[0]):\n",
        "        for df2_column in range(0, d_feature_map2.shape[1]):\n",
        "          start_row = df2_row * convstride2 # multiply with the stride\n",
        "          start_column = df2_column * convstride2\n",
        "          region = pooled_map1[start_row: start_row+filter2size, start_column: start_column+filter2size] # +'s are kernel size, e.g 2x2\n",
        "          d_filter2+= region * d_feature_map2[df2_row, df2_column]\n",
        "          d_pooled_map1[start_row: start_row+filter2size, start_column: start_column+filter2size] += filter2 * d_feature_map2[df2_row, df2_column]\n",
        "\n",
        "      # max pooling 1 backpropagation\n",
        "      d_feature_map1_activated = np.zeros_like(feature_map1_activated)\n",
        "      for pl_row in range(0, d_pooled_map1.shape[0]):\n",
        "        for pl_column in range(0, d_pooled_map1.shape[1]):\n",
        "          start_row = pl_row * poolingstride1 # multiply it with stride\n",
        "          start_column = pl_column * poolingstride1\n",
        "          end_row = start_row + poolingsize1 # add the pool size to get the regions other end\n",
        "          end_column = start_column + poolingsize1\n",
        "          region = feature_map1_activated[start_row:end_row, start_column: end_column]\n",
        "          max_index = np.unravel_index(np.argmax(region), region.shape)\n",
        "          max_pos = (start_row + max_index[0], start_column + max_index[1])\n",
        "          d_feature_map1_activated[max_pos] += d_pooled_map1[pl_row, pl_column]\n",
        "      d_feature_map1 = relu_derivative(d_feature_map1_activated)\n",
        "\n",
        "      # convolution layer 1 backpropagation\n",
        "      d_filter1 = np.zeros_like(filter1)\n",
        "      for df1_row in range(0, d_feature_map1.shape[0]):\n",
        "        for df1_column in range(0, d_feature_map1.shape[1]):\n",
        "          start_row = df1_row * convstride1\n",
        "          start_column = df1_column * convstride1\n",
        "          region = train_image_matrix[start_row: start_row+filter1size, start_column:start_column+filter1size]\n",
        "          d_filter1+= region * d_feature_map1[df1_row, df1_column]\n",
        "\n",
        "      # --- Parameter Updates ---\n",
        "      w2 -= learning_rate * dw2\n",
        "      b2 -= learning_rate * db2\n",
        "      w1 -= learning_rate * dw1\n",
        "      b1 -= learning_rate * db1\n",
        "      filter2 -= learning_rate * d_filter2\n",
        "      filter1 -= learning_rate * d_filter1\n",
        "  # Print loss after each epoch\n",
        "  print(f\"Epoch {epoch + 1}/{5}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# TESTING\n",
        "correct_predictions = 0  # Counter for correct predictions\n",
        "\n",
        "for i in range(len(test_images)):\n",
        "    test_image_matrix = test_images[i]\n",
        "\n",
        "   # First convolutional & max pooling layers\n",
        "    feature_map1 = conv2d_no_channel(test_image_matrix, filter1, stride=convstride1, padding=convpadding1)\n",
        "    feature_map1_activated = relu(feature_map1)\n",
        "    pooled_map1 = max_pooling(feature_map1_activated, pool_size=poolingsize1, stride=poolingstride1)\n",
        "\n",
        "    # Second convolutional & max pooling layers\n",
        "    feature_map2 = conv2d_no_channel(pooled_map1, filter2, stride=convstride2, padding=convpadding2)\n",
        "    feature_map2_activated = relu(feature_map2)\n",
        "    pooled_map2 = max_pooling(feature_map2_activated, pool_size=poolingsize2, stride=poolingstride2)\n",
        "\n",
        "    # Flatten the output for the neural network\n",
        "    flattened_vector = pooled_map2.flatten()\n",
        "\n",
        "    # Fully connected layer1\n",
        "    fc1_output = dense_layer(flattened_vector, w1, b1)\n",
        "    fc1_activated = relu(fc1_output)\n",
        "\n",
        "    # Fully connected layer2 (Output layer)\n",
        "    logits = dense_layer(fc1_activated, w2, b2)\n",
        "    output_probs = softmax(logits)\n",
        "\n",
        "    # Predicted class\n",
        "    predicted_class = np.argmax(output_probs)\n",
        "    true_class = np.argmax(test_labels[i])  # True label from test set\n",
        "\n",
        "    # Check if the prediction is correct\n",
        "    if predicted_class == true_class:\n",
        "        correct_predictions += 1\n",
        "\n",
        "# --- Calculate Accuracy ---\n",
        "accuracy = correct_predictions / len(test_images)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
